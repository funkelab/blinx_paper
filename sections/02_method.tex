\section{Method}

Consider a complex of fluorescent emitters, smaller than can be visually separated. Each emitter activates stochastically and independently of the others, producing a signal of fluctuating intensity over time \figref{fig:results:comparison}.
The goal of this method is to determine the true number of emitters \truen given the observed intensity trace.

\subsection{Model}

First, focusing on only a single timepoint $t$, we need an emission distribution to describe the relationship between the number of active emitters \y{t} and the observed intensity \x{t}. The individual intensities of multiple emitters are additive, and can be approximated by a log-normal distribution \cite{mutch_deconvolving_2007} \ie.
%
\begin{equation*}
  p(\x{t}|\y{t}, \mu, \sigma) =
    \frac{1}{\x{t}\sigma\sqrt{2\pi}}
    \exp \left(
      - \frac{|\log(\x{t}) - \y{t}\mu|^2}{2\sigma^2}
    \right)
  \text{,}
\end{equation*}

Where $\mu$ and $\sigma$ are the mean and standard deviation of the intensity of a single active emitter.

Next, to model the temporal fluctuations in intensity, observed in the trace \trace, we need a distribution to describe the change in the number of active emitters \states over time. To do this, we assume that the process is Markovian and the number of active emitters \y{t} at time $t$ is only dependent on the number of active emitters at the previous timepoint \y{t-1}. Breaking down the system to each individual emitter, we define the probability that an emitter that is active at time $t-1$ becoming inactive at time $t$ as \poff, conversely, we define the probability that an emitter inactive at time $t-1$ becoming active at time $t$ as \pon. Finally, assuming that all emitters share the same \poff and \pon, the transition distribution can be written as:
%
\begin{align*}
  p(\y{t} = y | \y{t-1}, \n, \pon, \poff) &=\\
	\sum_{a = 0}^{\y{t-1}}
    {a \choose \y{t-1}}
    &\pon
    {y - \y{t-1} + a \choose \n - \y{t-1}}
    \poff
\end{align*}
%
Where $a$ is the number of emitters changing state. Note that this distribution depends on the total number of available emitters
\n, as the probability of a change in the number of activate transmitters
depends on the total number of emitters available.

Expanding these distributions to account for entire intensity trace \trace and marginalizing over all possible \states, we can build a distribution for the probability of observing trace \trace given total number of emitters \n and simplifying the emission and transition distributions with $\parameters = (\pon, \poff, \mu, \sigma)$

\begin{align*}
  p(\trace|\n,\parameters) &=\\
    \sum_{\states}
      p(&\x{1}|\y{1}, \parameters)
      p(\y{1}|\n, \parameters)
      \prod_{t=2}^{T}
        p(\x{t}|\y{t}, \parameters)
        p(\y{t}|\y{t-1},\n, \parameters)
  \label{eq:method:likelihood}
\end{align*}

Maximum likelihood estimation can then be used to find the \parameters that best fit the observation \trace, given \n. 
Finally this process can be repeated for all possible values of n, and maximum likelihood estimation used to find the estimated number of emitters in the system \estimatedn.

\begin{equation}
    \estimatedn =
    \argmax{\n}
    \max_\parameters
    p(\trace|\n,\parameters)
  \text{,}
  \label{eq:method:optimization}
\end{equation}

\subsection{Inference}

Because there is no closed form solution to  \eqref{eq:method:optimization} we can use gradient ascent to find the optimal $\theta$ given \n.
The gradient of the likelihood function $p(\trace | \n, \theta)$ was calculated using JAX, an auto-differentiation library.
This was done recursively, as is standard for hidden Markov models, through the forward algorithm.
The likelihood $p(\trace | \n, \theta)$ was defined recursively through \pchain(\y{t}), the probability of observing the sequence of events 0 through $t$.
Therefore the probability of observing trace \trace is $ p(\trace|\n,\parameters) = \sum_{\y{T}} \pchain_T(\y{T})$ and, omitting \n and \parameters for simplicity, \pchain(\y{t})  is defined as:

\begin{align*}
  \pchain_{t}(\y{t}) &= p(\x{t}|\y{t}) \sum_{\y{t-1}} p(\y{t} | \y{t-1}) \pchain_{t-1}(\y{t-1})\\
  \pchain_{1}(\y{1}) &= p(\x{1}|\y{1}) p(\y{1})
\end{align*}

Because $\pchain_{t}$ depends multiplicatively on the previous $\pchain_{t-1}$,
the probability becomes vanishingly small for large times $t$, leading to
numerical stability problems. Further, because there is a marginalization over
\states at each $t$, the standard trick of moving to log space, is not
effective.

To alleviate this issue, we introduce a normalized recursive scheme, where
$\pchainnorm$ are normalized to sum up to one for all \y{}:
%
\begin{align*}
  \pchainnorm_t(\y{t}) &=
    \frac{1}{\norms_t}
    p(\x{t}|\y{t})
    \sum_{\y{t-1}}
      p(\y{t} | \y{t-1})
      \pchainnorm_{t-1}(\y{t-1})\\
  \pchainnorm_{1}(\y{1}) &=
    \frac{1}{\norms_1}
    p(\x{t}|\y{t})
    p(\y{t})\\
  \norms_t &=
    \sum_{\y{}}
    p(\x{t}|\y{})
    \sum_{\y{t-1}}
      p(\y{} | \y{t-1})
      \pchainnorm_{t-1}(\y{t-1})\\
  \norms_1 &=
    \sum_{\y{}}
    p(\x{1}|\y{})
    p(\y{1})\\
\end{align*}
%
Redefining $\pchain_{t}$ in terms of $\pchainnorm_{t}$:
%
\begin{equation*}
  \pchain_t(\y{t}) = \pchainnorm_t \prod_{t'=1}^t \norms_{t'}
\end{equation*}
Plugging back into the likelihood function, we find that the likelihood is simply the product of all $\beta$s 
\begin{align*}
  p(\trace|\n,\parameters)
    &= \sum_{\y{T}} \pchainnorm_{t}(\y{T}) \prod_{t'=1}^T \norms_{t'}\\
    &= \left( \prod_{t'=1}^T \norms_{t'} \right)
      \underbrace{\sum_{\y{T}} \pchainnorm_{t}(\y{T})}_{=1}
    = \prod_{t'=1}^T \norms_{t'}\\
\end{align*}
